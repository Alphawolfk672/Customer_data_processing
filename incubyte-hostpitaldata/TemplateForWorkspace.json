{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "incubyte-hostpitaldata"
		},
		"incubyte-hostpitaldata-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'incubyte-hostpitaldata-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:incubyte-hostpitaldata.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"incubyte-hostpitaldata-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://keshavgen2synapse.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/incubyte-hostpitaldata-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('incubyte-hostpitaldata-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/incubyte-hostpitaldata-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('incubyte-hostpitaldata-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://keshavgen2synapse.dfs.core.windows.net/hospital-data/Customer_data_src/customer_09102024.csv',\n        FORMAT = 'CSV',\n        HEADER_ROW=TRUE,\n        PARSER_VERSION = '2.0',\n        FIELDTERMINATOR='|'\n    ) \nWITH\n(\n    Column3 VARCHAR(255) COLLATE Latin1_General_100_BIN2_UTF8,\n    Column4 VARCHAR(18) COLLATE Latin1_General_100_BIN2_UTF8\n\n) AS [result]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkeshav672",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5228aa15-ce2e-4bb4-91c7-30dde2d79d17"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9072a345-2a6f-46b8-920e-bbcec129efc5/resourceGroups/incubyte/providers/Microsoft.Synapse/workspaces/incubyte-hostpitaldata/bigDataPools/sparkkeshav672",
						"name": "sparkkeshav672",
						"type": "Spark",
						"endpoint": "https://incubyte-hostpitaldata.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkeshav672",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"import re\r\n",
							"\r\n",
							"def extract_date(filename):\r\n",
							"    match = re.search(r'customer_(\\d{8})\\.csv', filename)\r\n",
							"    if match:\r\n",
							"        return match.group(1)\r\n",
							"    return None\r\n",
							"\r\n",
							"# Extract dates from filenames and find the latest\r\n",
							"file_dates = [(filename, extract_date(filename)) for filename in file_names if extract_date(filename)]\r\n",
							"\r\n",
							"# Sort by date to find the latest\r\n",
							"file_dates.sort(key=lambda x: x[1], reverse=True)\r\n",
							"latest_file = file_dates[0][0]  \r\n",
							"\r\n",
							"\r\n",
							"spark = SparkSession.builder.getOrCreate()\r\n",
							"df = spark.read.load('abfss://hospital-data@keshavgen2synapse.dfs.core.windows.net/Customer_data_src/*', format='csv',delimiter='|')\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"df.show()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 7
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4e7aa0cd-49e9-44c8-9036-dca4c5892024"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from delta.tables import DeltaTable\r\n",
							"\r\n",
							"# Load the existing Delta table\r\n",
							"delta_table = DeltaTable.forPath(spark, staging_table_path)\r\n",
							"\r\n",
							"# Load new data for upsert (assuming new data is loaded as a DataFrame)\r\n",
							"new_data_df = spark.read.csv(\"abfss://<container>@<storage_account>.dfs.core.windows.net/data/new_data.csv\", header=True, inferSchema=True)\r\n",
							"\r\n",
							"# Perform upsert using Delta's MERGE feature\r\n",
							"delta_table.alias(\"staging\").merge(\r\n",
							"    new_data_df.alias(\"new_data\"),\r\n",
							"    \"staging.customer_id = new_data.customer_id\"\r\n",
							").whenMatchedUpdateAll() \\\r\n",
							" .whenNotMatchedInsertAll() \\\r\n",
							" .execute()\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkkeshav672')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 3,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Customer_db')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "centralindia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/staging_table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkkeshav672",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "da86c121-b051-4333-a3ed-fce5c6b8002a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/9072a345-2a6f-46b8-920e-bbcec129efc5/resourceGroups/incubyte/providers/Microsoft.Synapse/workspaces/incubyte-hostpitaldata/bigDataPools/sparkkeshav672",
						"name": "sparkkeshav672",
						"type": "Spark",
						"endpoint": "https://incubyte-hostpitaldata.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkkeshav672",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# Initialize Spark session\r\n",
							"spark = SparkSession.builder \\\r\n",
							"    .appName(\"Customer_ETL\") \\\r\n",
							"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\r\n",
							"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\r\n",
							"    .getOrCreate()\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# Create Delta table for staging\r\n",
							"staging_table_path = \"abfss://hospital-data@keshavgen2synapse.dfs.core.windows.net/delta/staging_table\"\r\n",
							"\r\n",
							"# Create an empty Delta table if it doesn't exist\r\n",
							"spark.sql(f\"\"\"\r\n",
							"CREATE TABLE IF NOT EXISTS delta.`{staging_table_path}` (\r\n",
							"    Customer_Name STRING,\r\n",
							"    Customer_Id STRING,\r\n",
							"    Open_Date DATE,\r\n",
							"    Last_Consulted_Date DATE,\r\n",
							"    Vaccination_Id STRING,\r\n",
							"    Dr_Name STRING,\r\n",
							"    State STRING,\r\n",
							"    Country STRING,\r\n",
							"    post_code INT,\r\n",
							"    DOB DATE,\r\n",
							"    Is_Active STRING\r\n",
							") USING DELTA\r\n",
							"\"\"\")\r\n",
							"\r\n",
							"spark.sql(f\"\"\"\r\n",
							"    CREATE TABLE IF NOT EXISTS customer_staging\r\n",
							"    USING DELTA\r\n",
							"    LOCATION '{staging_table_path}'\r\n",
							"\"\"\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 67
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, DateType\r\n",
							"from pyspark.sql.functions import col,to_date\r\n",
							"from pyspark.sql import functions as F\r\n",
							"input_path = \"abfss://hospital-data@keshavgen2synapse.dfs.core.windows.net/Customer_data_src/customer_09102024.csv\"\r\n",
							"\r\n",
							"# Load CSV data into DataFrame\r\n",
							"df = spark.read.csv(input_path, header=True,sep='|')\r\n",
							"df_filter = df.withColumn(\"Open_Date\", to_date(col(\"Open_Date\"), \"yyyyMMdd\"))\r\n",
							"df_filter = df_filter.withColumn(\"Last_Consulted_Date\" , to_date(col(\"Last_Consulted_Date\"), \"yyyyMMdd\"))\r\n",
							"df_filter = df_filter.withColumn(\"DOB\", to_date(col(\"DOB\"), \"ddMMyyyy\"))\r\n",
							"df_filter = df_filter.withColumn(\"post_code\", F.lit(None).cast(\"INT\"))\r\n",
							"df_filter = df_filter.withColumn(\"Is_Active\", col(\"Is_Active \"))\r\n",
							"\r\n",
							"\r\n",
							"# Select the DataFrame with reordered columns\r\n",
							"df_final = df_filter.select(\"Customer_Name\", \"Customer_Id\", \"Open_Date\", \"Last_Consulted_Date\", \"Vaccination_Id\", \"Dr_Name\", \"State\", \"Country\", \"post_code\"\r\n",
							", \"DOB\", \"Is_Active\")\r\n",
							"df_final.show()\r\n",
							"#df_filter.show()\r\n",
							"\r\n",
							"#df_filter.explain()\r\n",
							"\r\n",
							"#df_filter.describe()\r\n",
							"# Write data to Delta staging table\r\n",
							"df_final.write.format(\"delta\").mode(\"append\").save(staging_table_path)"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_result = spark.sql(\"SELECT * FROM customer_staging\")\r\n",
							"#df_result.show()\r\n",
							"spark.sql(\"SHOW TABLES\").show()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 100
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_base_table')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8d645e71-337e-4540-bd64-9a6824eea250"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"# Initialize Spark session\r\n",
							"spark = SparkSession.builder \\\r\n",
							"    .appName(\"Optimized Create Base Tables from Country\") \\\r\n",
							"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\r\n",
							"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\r\n",
							"    .getOrCreate()\r\n",
							"\r\n",
							"# Define the path to your staging Delta table\r\n",
							"staging_table_path = \"abfss://<container>@<storage_account>.dfs.core.windows.net/delta/staging_table\"\r\n",
							"\r\n",
							"# Step 1: Load the Delta table into a DataFrame and create a temporary view\r\n",
							"df_staging = spark.read.format(\"delta\").load(staging_table_path)\r\n",
							"df_staging.createOrReplaceTempView(\"customer_staging\")\r\n",
							"\r\n",
							"# Step 2: Get distinct countries using SQL\r\n",
							"distinct_countries = spark.sql(\"SELECT DISTINCT Country FROM customer_staging\").collect()\r\n",
							"\r\n",
							"# Step 3: Create DataFrames for each country and store them in a dictionary\r\n",
							"country_dfs = {}\r\n",
							"for row in distinct_countries:\r\n",
							"    country = row['Country']\r\n",
							"    country_dfs[country] = spark.sql(f\"SELECT * FROM customer_staging WHERE Country = '{country}'\")\r\n",
							"\r\n",
							"# Step 4: Write each country's DataFrame to Delta table in one go\r\n",
							"for country, df_country in country_dfs.items():\r\n",
							"    # Define the path for the country-specific Delta table\r\n",
							"    country_table_path = f\"abfss://<container>@<storage_account>.dfs.core.windows.net/delta/customer_{country}\"\r\n",
							"    \r\n",
							"    # Create a new Delta table for each country\r\n",
							"    spark.sql(f\"\"\"\r\n",
							"        CREATE TABLE IF NOT EXISTS delta.`{country_table_path}` (\r\n",
							"            Customer_Name STRING,\r\n",
							"            Customer_Id STRING,\r\n",
							"            Open_Date DATE,\r\n",
							"            Last_Consulted_Date DATE,\r\n",
							"            Vaccination_Id STRING,\r\n",
							"            Dr_Name STRING,\r\n",
							"            State STRING,\r\n",
							"            Country STRING,\r\n",
							"            Post_Code INT,\r\n",
							"            DOB DATE,\r\n",
							"            Is_Active STRING\r\n",
							"        ) USING DELTA\r\n",
							"    \"\"\")\r\n",
							"    \r\n",
							"    # Write the country-specific DataFrame to the Delta table\r\n",
							"    df_country.write.format(\"delta\").mode(\"append\").save(country_table_path)\r\n",
							"\r\n",
							"    print(f\"Created and populated table for country: {country}\")\r\n",
							"\r\n",
							"# Optionally: Show tables created\r\n",
							"spark.sql(\"SHOW TABLES\").show()\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"from delta.tables import DeltaTable\r\n",
							"\r\n",
							"deltaTable = DeltaTable.forPath(spark, \"<base_table_path>\")\r\n",
							"\r\n",
							"# Assuming df_staging contains the new data to be inserted/updated\r\n",
							"deltaTable.alias(\"base\").merge(\r\n",
							"    df_staging.alias(\"staging\"),\r\n",
							"    \"base.Customer_Id = staging.Customer_Id\"\r\n",
							").whenMatchedUpdateAll() \\\r\n",
							" .whenNotMatchedInsertAll() \\\r\n",
							" .execute()\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		}
	]
}