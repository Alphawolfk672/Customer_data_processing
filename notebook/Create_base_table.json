{
	"name": "Create_base_table",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4c73720e-feef-4a5d-b421-255cd7c9f7cf"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"microsoft": {
						"language": "python"
					}
				},
				"source": [
					"%%pyspark\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"# Initialize Spark session\r\n",
					"spark = SparkSession.builder \\\r\n",
					"    .appName(\"Optimized Create Base Tables from Country\") \\\r\n",
					"    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\r\n",
					"    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\r\n",
					"    .getOrCreate()\r\n",
					"\r\n",
					"# Define the path to your staging Delta table\r\n",
					"staging_table_path = \"abfss://<container>@<storage_account>.dfs.core.windows.net/delta/staging_table\"\r\n",
					"\r\n",
					"# Step 1: Load the Delta table into a DataFrame and create a temporary view\r\n",
					"df_staging = spark.read.format(\"delta\").load(staging_table_path)\r\n",
					"df_staging.createOrReplaceTempView(\"customer_staging\")\r\n",
					"\r\n",
					"# Step 2: Get distinct countries using SQL\r\n",
					"distinct_countries = spark.sql(\"SELECT DISTINCT Country FROM customer_staging\").collect()\r\n",
					"\r\n",
					"# Step 3: Create DataFrames for each country and store them in a dictionary\r\n",
					"country_dfs = {}\r\n",
					"for row in distinct_countries:\r\n",
					"    country = row['Country']\r\n",
					"    country_dfs[country] = spark.sql(f\"SELECT * FROM customer_staging WHERE Country = '{country}'\")\r\n",
					"\r\n",
					"# Step 4: Write each country's DataFrame to Delta table in one go\r\n",
					"for country, df_country in country_dfs.items():\r\n",
					"    # Define the path for the country-specific Delta table\r\n",
					"    country_table_path = f\"abfss://<container>@<storage_account>.dfs.core.windows.net/delta/customer_{country}\"\r\n",
					"    \r\n",
					"    # Create a new Delta table for each country\r\n",
					"    spark.sql(f\"\"\"\r\n",
					"        CREATE TABLE IF NOT EXISTS delta.`{country_table_path}` (\r\n",
					"            Customer_Name STRING,\r\n",
					"            Customer_Id STRING,\r\n",
					"            Open_Date DATE,\r\n",
					"            Last_Consulted_Date DATE,\r\n",
					"            Vaccination_Id STRING,\r\n",
					"            Dr_Name STRING,\r\n",
					"            State STRING,\r\n",
					"            Country STRING,\r\n",
					"            Post_Code INT,\r\n",
					"            DOB DATE,\r\n",
					"            Is_Active STRING\r\n",
					"        ) USING DELTA\r\n",
					"    \"\"\")\r\n",
					"    \r\n",
					"    # Write the country-specific DataFrame to the Delta table\r\n",
					"    df_country.write.format(\"delta\").mode(\"append\").save(country_table_path)\r\n",
					"\r\n",
					"    print(f\"Created and populated table for country: {country}\")\r\n",
					"\r\n",
					"# Optionally: Show tables created\r\n",
					"spark.sql(\"SHOW TABLES\").show()\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"from delta.tables import DeltaTable\r\n",
					"\r\n",
					"deltaTable = DeltaTable.forPath(spark, \"<base_table_path>\")\r\n",
					"\r\n",
					"# Assuming df_staging contains the new data to be inserted/updated\r\n",
					"deltaTable.alias(\"base\").merge(\r\n",
					"    df_staging.alias(\"staging\"),\r\n",
					"    \"base.Customer_Id = staging.Customer_Id\"\r\n",
					").whenMatchedUpdateAll() \\\r\n",
					" .whenNotMatchedInsertAll() \\\r\n",
					" .execute()\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": null
			}
		]
	}
}